{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opposite-terrorist",
   "metadata": {},
   "source": [
    "Wahlpflichtfach Künstliche Intelligenz I: Praktikum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-standard",
   "metadata": {},
   "source": [
    "# 08 Scikit-learn - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-stand",
   "metadata": {},
   "source": [
    "__Scikit-learn__ (auch als __sklearn__ bekannt) ist ein Open-Source Software-Bibliotheke zum maschinellen Lernen in Python. Sie erfreut sich großer Beliebtheit und wird aktiv gewartet. Die Bibliothek bietet verschiedene Klassifikations-, Regressions- und Clustering-Algorithmen an. Darüber hinaus sind auch Algorithmen zur Modellauswahl, Dimensionsreduktion und Datenvorverarbeitung in sklearn enthalten. \n",
    "\n",
    "In diesem Noteboob beschäftigen wir uns (erneut) mit der Datenvorverarbeitung (Data Preparation) und gehen dabei auf die folgenden Themen ein:\n",
    "- Imputation\n",
    "- Skalierung\n",
    "- Dimensionreduktion\n",
    "- Pipelines\n",
    "- Feature Union\n",
    "- Column Transformations\n",
    "\n",
    "Die Dokumentation zu scikit-learn ist [hier](https://scikit-learn.org/stable/index.html) zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "settled-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-pizza",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "Unter Imputation ist das Vervollständigen von fehlenden Werten (NaNs) zu verstehen. Wie in pandas auch gibt es verschiedene Methoden um fehlende Werte in sklearn zu ersetzen. Weitere Informationen sind [hier](https://scikit-learn.org/stable/modules/impute.html) zu finde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satisfied-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = np.array([[1, 2], [np.nan, 3], [7, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-prisoner",
   "metadata": {},
   "source": [
    "### Eindimensionale Imputation\n",
    "In der eindimensonalen Imputation werden die Werte je Spalte ersetzt. Die Klasse [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) bietet dafür grundlegende Strategien. Fehlende Werte können mit einem bereitgestellten konstanten Wert oder mit statistischen Werten (Mittelwert, Median oder häufigster Wert) jeder Spalte, in der sich die fehlenden Werte befinden, ersetzt werden. Diese Klasse erlaubt auch verschiedene Kodierungen für fehlende Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "raising-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# define different imputer\n",
    "mean_imputer = SimpleImputer()\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the imputer has to be fitted to the data, so either call first fit and then transform \n",
    "# or call fit_transform to do it in one step\n",
    "mean_imputer.fit(nan_data)\n",
    "mean_imputer.transform(nan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_imputer.fit_transform(nan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_nan_data = np.array([[np.nan, 5], [8, 2], [6, 6]])\n",
    "mean_imputer.transform(different_nan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-perception",
   "metadata": {},
   "source": [
    "__Brainstorming:__\n",
    "<details>\n",
    "<summary>Wieso wurde np.nan durch 4 ersetzt?</summary>\n",
    "Da der mean_inputer zuvor auf den anderen Daten \"trainiert\" wurde. \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Wann kann dieses Verhalten von Vorteil sein?</summary>\n",
    "Ein Vorteil ist es, dass die Ersetzungs-Strategien bzw. exakten Werte, die während des Trainings verwendet wurden, auch zur Test-Zeit bzw. im Live-Betrieb verwendet werden können. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_imputer.transform(different_nan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-cattle",
   "metadata": {},
   "source": [
    "Es ist auch möglich andere Werte als `np.nan` zu ersetzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fischers_fritz = [['Fischers', '', 'fischt', 'frische', 'Fische'],\n",
    "                  ['Frische', 'Fische', 'fischt', 'Fischers', '']]\n",
    "\n",
    "string_imputer = SimpleImputer(missing_values='', strategy='constant', fill_value='Fritz')\n",
    "string_imputer.fit_transform(fischers_fritz)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-memorabilia",
   "metadata": {},
   "source": [
    "### Mehrdimensionale Variante\n",
    "Die mehrdimensionalen Variante ist dahingegen deutlich anspruchsvoller. Grob zusammengefasst wird jeder fehlende Wert als Funktion anderer Merkmale modelliert und diese Schätzung zur Imputation verwendet. Dieser Vorgang wird dann einige Male wiederholt,, bevor die finalen Ersetzungen vorgenommen werden. Dieses Verhalten wird vom [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) implementiert. \n",
    "\n",
    "__Achtung:__ Diese Klasse ist noch experimentell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spanish-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-mistress",
   "metadata": {},
   "source": [
    "Zuerst erstellen wir uns ein kleines Dummy-Dataset, wo die Werte der einzelnen Spalten eine klare Beziehung zueinander haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 11, dtype=\"float\")\n",
    "y = x * x \n",
    "data = np.array([x, y])\n",
    "data[(0, 1)] = np.nan\n",
    "data[(0, 6)] = np.nan\n",
    "data[(1, 3)] = np.nan\n",
    "data[(1, 9)] = np.nan\n",
    "data = data.T\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-birthday",
   "metadata": {},
   "source": [
    "Anschließend können wir wieder mit `fit_transform` die Daten ersetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "IterativeImputer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-watts",
   "metadata": {},
   "source": [
    "Die Ersetzungen des `SimpleImputer`s sehen hingegen folgendermaßen aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleImputer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-shield",
   "metadata": {},
   "source": [
    "__Brainstorming:__\n",
    "<details>\n",
    "    <summary>Welche Vorteile bringt die mehrdimensionale Variante?</summary>\n",
    "    Ein großer Vorteil ist, dass die nicht vorhandenen Daten abhängig von  anderen Daten ersetzt werden. Dies ist häufig besser, da es Abhängigkeiten zwischen den Daten geben kann. Beispiel: Größe und Gewicht von Personen.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-steam",
   "metadata": {},
   "source": [
    "In den folgenden drei Codezeilen, wird die iterative Arbeitsweise des Algorithmus sichtbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "IterativeImputer(max_iter=1).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "IterativeImputer(max_iter=2).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "IterativeImputer(max_iter=3).fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-teens",
   "metadata": {},
   "source": [
    "## Skalierung\n",
    "\n",
    "### Standardisierung\n",
    "Unter Standardisiserung ist eine Transformation der Eingabedaten zu verstehen, so dass die resultierenden Daten eine Mittelwert von 0 und eine Varianz von 1 haben. Die resultierenden Daten sind also normalverteilt. Dies ist besonders hilfreich, wenn alle Variablen unterschiedlich skaliert sind. \n",
    "\n",
    "__Beispiel:__ Ein Datensatz enthält die Werte Größe in Metern und Gewicht in Kilogramm von Menschen. Die Varianz der Variablen wird deutlich unterschiedlich sein, da die Größe sich auf einer Skala von 0,1 m bis 2,8 m befindet und das Gewicht auf einer Skala von 0,5 kg bis 600 kg.\n",
    "\n",
    "In sklearn wird für die Standardisierung die Klasse [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) verwendet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaling_data = np.array([[1.79, 79.5], \n",
    "                         [1.60, 53.2], \n",
    "                         [2.59, 150], \n",
    "                         [1.73, 70.7], \n",
    "                         [1.50, 46.7], \n",
    "                         [1.75, 113.0], \n",
    "                         [1.93, 247.2]])\n",
    "print(f\"Mittelwerte pro Feature: {scaling_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {scaling_data.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(scaling_data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mittelwerte pro Feature: {scaled_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {scaled_data.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-slide",
   "metadata": {},
   "source": [
    "__Brainstorming:__\n",
    "<details>\n",
    "<summary>Wieso ist der Mittelwert nicht bei 0?</summary>\n",
    "Das sind Berechnungsfehler, die vernachlässigt werden können. e-17 ist eine verdammt kleine Zahl.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-theory",
   "metadata": {},
   "source": [
    "Wenn die Daten nicht zentriert werden sollen, kann dies mit dem Parameter `with_mean=False` verhindert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_center_scaler = StandardScaler(with_mean=False)\n",
    "non_centric_data = not_center_scaler.fit_transform(scaling_data)\n",
    "\n",
    "print(f\"Mittelwerte pro Feature: {non_centric_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {non_centric_data.std(axis=0)}\")\n",
    "non_centric_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-calculator",
   "metadata": {},
   "source": [
    "Ebenso ist es möglich die Daten nicht zu skalieren, um die Varianz zu behalten. Dafür muss der Konstruktor mit dem Parameter `with_std=False` aufgrufen werde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_scaling_scaler = StandardScaler(with_std=False)\n",
    "non_scaled_data = not_scaling_scaler.fit_transform(scaling_data)\n",
    "\n",
    "print(f\"Mittelwerte pro Feature: {non_scaled_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {non_scaled_data.std(axis=0)}\")\n",
    "non_scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-confirmation",
   "metadata": {},
   "source": [
    "### Skalierung der Features in einem bestimmten Bereich\n",
    "Alternativ kann die Skalierung von Featuren so erfolgen, dass sie zwischen einem vorgegebenen Minimal- und Maximalwert liegen, oft zwischen Null und Eins, oder so, dass der maximale Absolutwert jedes Features auf Einheitsgröße skaliert wird. Dies kann mit [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#) bzw. [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#) erreicht werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_data = np.array([[-1, 2], \n",
    "                         [-0.5, 6], \n",
    "                         [0, 10], \n",
    "                         [1, 18]])\n",
    "\n",
    "zero_one_scaler = MinMaxScaler()\n",
    "zero_one_scaler.fit_transform(min_max_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-printer",
   "metadata": {},
   "source": [
    "Um den Bereich zu ändern kann im Konstruktor der Parameter `feature_range` spezifiziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_one_one_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "minus_one_one_scaler.fit_transform(min_max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_abs_data = np.array([[ 1., -1.,  2.],\n",
    "                         [ 2.,  0.,  0.],\n",
    "                         [ 0.,  1., -1.]])\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "max_abs_scaler.fit_transform(max_abs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-trash",
   "metadata": {},
   "source": [
    "### Probleme mit Ausreißern\n",
    "Die drei vorgestellten Klassen können bei der Skalierung nicht besonders gut mit Ausreißern (outlier) umgehen. Dieses Problem wird [hier](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py) dargestellt.\n",
    "\n",
    "Ein Skaler, der mit Ausreißern funktioniert ist der [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "outlier_data = np.array([[2, 4, 1, 27, 3, 4, 1, 3, 3, 2],\n",
    "                [100, 92, 87, 94, 95, 83, 177, 84, 99, 89]]).T\n",
    "\n",
    "robust_scaler = RobustScaler(quantile_range=(25, 75))\n",
    "robust_scaler.fit_transform(outlier_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler.center_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-basket",
   "metadata": {},
   "source": [
    "Im Vergleich dazu würde der StandardScaler die Daten folgendermaßen skalieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(outlier_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-rover",
   "metadata": {},
   "source": [
    "__Aufgabe:__ \n",
    "<details>\n",
    "<summary>Wie werden die neuen Werte berechnet?</summary>\n",
    "Zuerst müssen der Median und die Quantile pro Spalte bestimmt werden. Dann können die neuen Werte wie folgt berechnet werden:\n",
    "    $$x_{neu} = \\frac{x_{alt} - median}{quantile_{upper} - quantil_{lower}}$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-hollow",
   "metadata": {},
   "source": [
    "## Dimensionsreduktion\n",
    "Unter Dimensionsreduktion ist die Reduzierung der Anzahl der Zufallsvariablen zu verstehen. Dies kann sowohl für die Visualisierung als auch fürs maschinelle Lernen hilfreich sein.\n",
    "\n",
    "Im Folgendem werden wir den Iris-Datensatz (Iris = Schwertlilie) verwenden. Er besteht aus 4 Features (Sepal Länge, Sepal Breite, Petal Länge und Petal Breite) und hat 3 Klassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris(as_frame=True)\n",
    "iris_df = iris.frame\n",
    "iris_df['Label'] = iris.target_names[iris_df['target']]\n",
    "iris_df = iris_df.drop('target', axis=1)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.pairplot(iris_df, hue=\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-hotel",
   "metadata": {},
   "source": [
    "### Principial Component Analyse (PCA)\n",
    "PCA wird verwendet, um einen multidimensionalen Datensatz in eine Menge von aufeinanderfolgenden orthogonalen Komponenten zu zerlegen, die einen maximalen Anteil der Varianz erklären. Wie auch die vorherigen Algorithmen ist [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) als Transformer-Objekt implementiert. Das heißt es muss erst mit `fit()` trainiert werden. Anschließend kann es dann auf die Daten angewandt werden und die $n$ Feature finde, die den größten Anteil der Varainz in den Daten erklären."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# we want to have the two components with the highest variance\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(iris.data)\n",
    "reduced_iris = pca.transform(iris.data)\n",
    "\n",
    "# show the two components with the highest variance\n",
    "ax_pca = sns.scatterplot(x=reduced_iris[:, 0], y=reduced_iris[:, 1], hue=iris_df['Label'])\n",
    "ax_pca.set_xlabel('Dimension 1')\n",
    "ax_pca.set_ylabel('Dimension 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-public",
   "metadata": {},
   "source": [
    "__Brainstorming:__\n",
    "<details>\n",
    "<summary>Was könnte eine Schwäche von PCA im Kontext einer Klassifikationsaufgabe sein?</summary>\n",
    "PCA versucht die größte Varianz in der Gesamtheit der Daten zu finden. Dabei beachtet PCA aber nicht die Varianz zwischen den einzelnen Klassen. Dies kann dazu führen, dass suboptimale Features für die Klassifizeirung ausgewählt werden. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-claim",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "[LDA]() verwendet im Gegensatz zu PCA auch die Klassenzugehörigkeit. Dabei versucht es die Features zu ermitteln, die den größten Einfluss auf die Varianz zwischen den Klassen haben. Da LDA die Klassenzugehörigkeit verwendet, handelt es sich dabei um eine supervised Methode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda.fit(iris.data, iris.target)\n",
    "lda_reduced_iris = lda.transform(iris.data)\n",
    "\n",
    "# show the two components with the highest variance between classes\n",
    "ax_lda = sns.scatterplot(x=lda_reduced_iris[:, 0], y=lda_reduced_iris[:, 1], hue=iris_df['Label'])\n",
    "ax_lda.set_xlabel('Dimension 1')\n",
    "ax_lda.set_ylabel('Dimension 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-kelly",
   "metadata": {},
   "source": [
    "### Vergleich von PCA und LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
    "sns.scatterplot(x=reduced_iris[:, 0], y=reduced_iris[:, 1], hue=iris_df['Label'], ax=ax1)\n",
    "ax1.set_xlabel('Dimension 1')\n",
    "ax1.set_ylabel('Dimension 2')\n",
    "ax1.set_title('PCA')\n",
    "sns.scatterplot(x=lda_reduced_iris[:, 0], y=lda_reduced_iris[:, 1], hue=iris_df['Label'], ax=ax2)\n",
    "ax2.set_xlabel('Dimension 1')\n",
    "ax2.set_ylabel('Dimension 2')\n",
    "ax2.set_title('LDA')\n",
    "fig.suptitle('Comparison of PCA and LDA')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-monte",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "[Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) kann verwendet werden, um mehrere Kalkulatoren zu einem zu verketten. Dies ist nützlich, da es oft eine feste Abfolge von Schritten bei der Verarbeitung der Daten gibt, z. B. Merkmalsauswahl, Normalisierung und Klassifizierung. Pipeline dient hier mehreren Zwecken:\n",
    "\n",
    "1) __Bequemlichkeit und Verkapselung__\n",
    "> Sie müssen `fit` und `predict` nur einmal auf Ihren Daten aufrufen, um eine ganze Sequenz von Kalkulatoren anzupassen.\n",
    "\n",
    "2) __Gemeinsame Parameterauswahl__\n",
    "> Sie können eine Rastersuche (Grid Search) über die Parameter aller Kalkulatoren in der Pipeline auf einmal durchführen. Darauf gehen wir später weiter ein.\n",
    "\n",
    "3) __Sicherheit__\n",
    "> Pipelines helfen dabei, ein Durchsickern von Statistiken aus Ihren Testdaten in das trainierte Modell bei der Kreuzvalidierung (darauf gehen wir auch später noch ein) zu vermeiden, indem sichergestellt wird, dass die gleichen Stichproben zum Trainieren der Transformatoren und Vorhersager verwendet werden.\n",
    "\n",
    "Alle Kalkulatoren in einer Pipeline, außer dem letzten, müssen Transformatoren sein (d. h. sie müssen eine `transform`-Methode haben). Der letzte Kalkulatoren kann ein beliebiger Typ sein (Transformator, Klassifikator usw.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-blank",
   "metadata": {},
   "source": [
    "### Konstruktor\n",
    "Die [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) wird mit einer Liste von (Schlüssel, Wert)-Paaren aufgebaut, wobei der Schlüssel eine Zeichenkette ist, die den Namen enthält, den Sie diesem Schritt geben möchten, und der Wert ein Kalkulator ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# first, create a dictionary with all the estimators we want to have in the pipeline, each step needs a name\n",
    "estimators = [('imputer', SimpleImputer()), ('scaler', StandardScaler())]\n",
    "\n",
    "# then, pass the estimators to the Pipeline constructor\n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't want to name the estimators explicit you can use the make_pipeline function\n",
    "from sklearn.pipeline import make_pipeline\n",
    "make_pipeline(SimpleImputer(), StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-parliament",
   "metadata": {},
   "source": [
    "### Parameter der Kalkulatoren setzen\n",
    "Auf die Parameter der Kalkulatoren in der Pipeline kann mit der Syntax `<estimator>__<parameter>` (_2 Unterstriche_) zugegriffen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(imputer__strategy='constant', imputer__fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-rover",
   "metadata": {},
   "source": [
    "Nachdem die Parameter gesetzt wurden können wir die Pipeline mit dummy-Werten testen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_data = [[1, 4], [5, 8], [np.nan, 5], [3, np.nan]]\n",
    "pipe.fit_transform(pipe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-pointer",
   "metadata": {},
   "source": [
    "Um andere Parameter zu verwenden, müssen diese wieder mit `set_params()` gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset to default parameters\n",
    "pipe.set_params(imputer__strategy='mean', imputer__fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit_transform(pipe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-drama",
   "metadata": {},
   "source": [
    "Des Weiteren ist es möglich auch einzelne Schritte komplett zu ersetzen. Dazu wird ebenfalls wieder `set_params()` verwendet und der Name des Schrittes angegeben, der ersetzt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(scaler=MaxAbsScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit_transform(pipe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-girlfriend",
   "metadata": {},
   "source": [
    "Wenn ein Schritt in der Pipeline nicht mehr ausgeführt werden soll, kann dies durch das Übergeben des Wertes `'passthrough'` erreicht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(imputer='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit_transform(pipe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-chorus",
   "metadata": {},
   "source": [
    "## Feature Union\n",
    "[FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) kombiniert mehrere Transformator-Objekte zu einem neuen Transformator, der deren Ausgabe kombiniert. Eine FeatureUnion nimmt eine Liste von Transformator-Objekten auf. Während der Anpassung wird jeder von ihnen unabhängig an die Daten angepasst. Die Transformatoren werden parallel angewendet und die von ihnen ausgegebenen Feature-Matrizen werden nebeneinander zu einer größeren Matrix verkettet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-dinner",
   "metadata": {},
   "source": [
    "### Konstruktor\n",
    "Eine FeatureUnion wird aus einer Liste von (Schlüssel, Wert)-Paaren aufgebaut, wobei der Schlüssel der Name ist, den Sie einer gegebenen Transformation geben möchten und der Wert ein Kalkulator ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n",
    "combined = FeatureUnion(estimators)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-simpson",
   "metadata": {},
   "source": [
    "Auch hier ist es wieder möglich die Schritte nicht explizit zu benennen. Dafür kann die Methode `make_union()` verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_union\n",
    "make_union(PCA(), KernelPCA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = [[1, 4, 3], [5, 8, 7], [2, 5, 4], [3, 6, 5]]\n",
    "combined.fit_transform(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-president",
   "metadata": {},
   "source": [
    "### Parameter der Transformer setzen\n",
    "Auch hier können die Parameter der Transformer wieder über die Methode `set_params()` gesetzt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.set_params(linear_pca__n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-segment",
   "metadata": {},
   "source": [
    "Um einen Schritt zu ignorieren muss an den entsprechenden Schrittname der Wert `'drop'` übergeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.set_params(kernel_pca='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-bubble",
   "metadata": {},
   "source": [
    "## Column Transformations\n",
    "Viele Datensätze enthalten Merkmale verschiedener Typen, z. B. Text, Fließkommazahlen und Datumsangaben, wobei jeder Merkmalstyp separate Vorverarbeitungs- oder Featureextraktionsschritte erfordert. Oft ist es am einfachsten, die Daten vor der Anwendung von scikit-learn-Methoden vorzuverarbeiten, zum Beispiel mit __Pandas__. \n",
    "\n",
    "Allerdings kann die Verarbeitung der Daten vor der Übergabe an scikit-learn aus einem der folgenden Gründe problematisch sein:\n",
    "1) Das Einbeziehen von Statistiken aus Testdaten in die Präprozessoren macht die Ergebnisse der Kreuzvalidierung unzuverlässig (bekannt als Datenleck), zum Beispiel im Fall von Skalierern oder der Imputierung fehlender Werte.\n",
    "\n",
    "2) Sie möchten möglicherweise die Parameter der Vorverarbeitung in eine Parametersuche einbeziehen.\n",
    "\n",
    "Der [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) hilft bei der Durchführung verschiedener Transformationen für verschiedene Spalten der Daten, innerhalb einer Pipeline, die sicher vor Datenlecks ist und die parametrisiert werden kann.\n",
    "\n",
    "### Dummy-Daten erstellen\n",
    "Zuerst erstellen wir uns ein Datensatz der neben nummerischen Daten auch Text enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "judicial-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data_raw = pd.DataFrame(\n",
    "    {\n",
    "        'city': ['London', 'London', 'Paris', 'Sallisaw'],\n",
    "        'title': [\"His Last Bow\", \"How Watson Learned the Trick\", \"A Moveable Feast\", \"The Grapes of Wrath\"],\n",
    "        'expert_rating': [5, 3, 4, 5],\n",
    "        'user_rating': [4, 5, 4, 3]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-choice",
   "metadata": {},
   "source": [
    "__Brainstorming:__\n",
    "\n",
    "<details>\n",
    "<summary>Wie können wir die Textdaten verwenden?</summary>\n",
    "Um die Textdaten fürs maschinelle Lernen zu verwenden müssen diese in Zahlen umgewandelt werden. In diesem Fall kann die Stadt beispielsweise One-Hot encoded werden und für den Titel können wir die vorkommenden Wörter zählen (im Seminar werden wir noch genauer darauf eingehen).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-ballot",
   "metadata": {},
   "source": [
    "### Konstruktor\n",
    "Wir wollen auf die Spalte `'city'` den [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) anwenden und auf die Spalte `'title'` den [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "column_trans = ColumnTransformer([('city_category', OneHotEncoder(dtype='int'),['city']),\n",
    "                                  ('title_bow', CountVectorizer(), 'title')])\n",
    "\n",
    "column_trans.fit(book_data_raw) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-stanley",
   "metadata": {},
   "source": [
    "Die Namen der Features bekommt man über die Methode `get_feature_names()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.transform(book_data_raw).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-asset",
   "metadata": {},
   "source": [
    "Alle Features, die nicht an einen Transformator übergeben wurden, werden defaultmäßig nicht mit ausgegeben. Um dieses Verhalten zu verändern, kann dem Konstruktor ein Wert für den Parameter `remainder` übergeben werden.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer([('city_category', OneHotEncoder(dtype='int'),['city']),\n",
    "                                  ('title_bow', CountVectorizer(), 'title')],\n",
    "                                remainder='passthrough')\n",
    "\n",
    "column_trans.fit(book_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.transform(book_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-afternoon",
   "metadata": {},
   "source": [
    "Auch hier gibt es wieder die Möglichkeit, die Namen für die einzelnen Schritte nicht explizit zu vergeben. Dafür kann die Methode `make_column_transformer()` verwendet werden. Außerdem kann man auch einen Transformer an den Parameter `remainder` übergeben. Dieser wird dann auf alle übrigen Spalten angewandt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(), ['city']),\n",
    "    (CountVectorizer(), 'title'),\n",
    "    remainder=MinMaxScaler()\n",
    ")\n",
    "column_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.fit_transform(book_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-crash",
   "metadata": {},
   "source": [
    "## Generelle Funktionsweise\n",
    "Wie Ihr bestimmt bereits bemerkt habt sind die Schritte immer wieder die gleichen:\n",
    "1. __Konstruktor:__ Erstellen des Algorithmus. Die meisten Algorithmen können in diesem Schritt parametrisiert werden.\n",
    "2. __fit():__ Training des Algorithmus anhand der übergebenden Daten.\n",
    "3. __transform():__ Mit der `transform()`-Methode kann der Agorithmus anschließend die Transformationen für die übergebenen Werte druchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-hydrogen",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Wahlpflichtach Künstliche Intelligenz I: Praktikum "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
